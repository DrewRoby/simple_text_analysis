{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "import pprint\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of this comes from this link: https://stackabuse.com/text-summarization-with-nltk-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = ''\n",
    "participants = []\n",
    "pp = pprint.PrettyPrinter(indent=2, width=170)\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "with open('HUBG-Q3-Earnings-Transcript.txt') as txt:\n",
    "    raw = txt.read()\n",
    "\n",
    "with open('participants.txt') as part:\n",
    "    for line in part:\n",
    "        line = line.replace('\\n','')\n",
    "        participants.append(line)\n",
    "\n",
    "def stripStopwords(sentence):\n",
    "    words = nltk.wordpunct_tokenize(sentence.lower())\n",
    "    out = []\n",
    "    for word in words:\n",
    "        if not word in stop_words:\n",
    "           out.append(word)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.wordpunct_tokenize(raw)\n",
    "text = nltk.Text(tokens)\n",
    "words = [w.lower() for w in text]\n",
    "vocab = sorted(set(words))\n",
    "word_tagged = nltk.pos_tag(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis (skip me for now):\n",
    "http://www.nltk.org/howto/sentiment.html\n",
    "Not sure what good this does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs = subj_docs[80:100]\n",
    "train_obj_docs = obj_docs[:80]\n",
    "test_obj_docs = obj_docs[80:100]\n",
    "training_docs = train_subj_docs + train_obj_docs\n",
    "testing_docs = test_subj_docs + test_obj_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "len(unigram_feats)\n",
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy:0.8\n",
      "F-measure [obj]:0.8\n",
      "F-measure [subj]:0.8\n",
      "Precision [obj]:0.8\n",
      "Precision [subj]:0.8\n",
      "Recall [obj]:0.8\n",
      "Recall [subj]:0.8\n"
     ]
    }
   ],
   "source": [
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)\n",
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "        print('{0}:{1}'.format(key,value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the earnings call..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'content': \"Hello, and welcome to the Hub Group Third Quarter 2018 Earnings Conference Call. Dave Yeager, Hub's CEO, Don Maltby, Hub's President and Chief Operating Officer, and Terri Pizzuto are joining me on the call. At this time, all participants are in a listen-only mode. A brief question-and-answer session will follow the formal presentation. In order for everyone to have an opportunity to participate, please limit your inquiries to one primary and one follow-up question.\"},\n",
       " 2: {'content': \"Any forward-looking statements made during the course of the call or contained in the release represent the company's best good faith judgment as to what may happen in the future. Statements that are forward-looking can be identified by the use of words such as believe, expect, anticipate and project and variations of these words. Please review the cautionary statements in the release. In addition, you should refer to the disclosures in the company's Form 10-K and other SEC filings regarding factors that could cause actual results to differ materially from those projected in the forward-looking statements. As a reminder, this conference is being recorded.\"},\n",
       " 3: {'content': 'It is now my pleasure to turn the call over to your host, Dave Yeager. Sir, you may begin.'},\n",
       " 31: {'content': 'The first question in the queue comes from Scott Group with Wolfe Research. Your line is open, sir.'},\n",
       " 69: {'content': 'Thank you. The next question in the queue comes from Ben Hartford of Baird. Your line is open, sir.'},\n",
       " 93: {'content': 'The next question in the queue comes from Justin Long with Stephens. Please proceed.'},\n",
       " 125: {'content': 'We have our next question from Todd Fowler with KeyBanc.'},\n",
       " 174: {'content': 'The next question in the queue comes from David Ross with Stifel. Your line is open, sir. Please begin.'},\n",
       " 201: {'content': 'Thank you. The next question comes from Brian Ossenbeck with JPMorgan. Your line is open.'},\n",
       " 219: {'content': 'Thank you. The next question comes from Ravi Shanker with Morgan Stanley. Please proceed.'},\n",
       " 249: {'content': 'Thank you. The next question comes from Jason Seidl with Cowen & Company. Please proceed with your question.'},\n",
       " 290: {'content': 'Thank you. The next question in the queue comes from Matt Brooklier from Buckingham Research.'},\n",
       " 321: {'content': 'Okay. There are no further questions in the queue. I will turn the call over to Mr. Yeager for any final remarks.'},\n",
       " 325: {'content': \"Thank you, ladies and gentlemen. This concludes today's teleconference. Thank you for participating. You may now disconnect.\"}}"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras = raw.split(\"\\n\\n\")\n",
    "transcript_dict = dict()\n",
    "last_speaker = ''\n",
    "for para in paras:\n",
    "    #If para is the name of one of the participants, add that name as a key to a dictionary, and add a dictionary as its value\n",
    "    if para in participants:\n",
    "        transcript_dict.setdefault(para,dict())\n",
    "        last_speaker = para\n",
    "    else:\n",
    "        #Add each subsequent paragraph to that person's own dictionary\n",
    "        transcript_dict[last_speaker].setdefault(paras.index(para),{'content':para})\n",
    "transcript_dict.pop('Operator')\n",
    "# pp.pprint(transcript_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "for participant in transcript_dict:\n",
    "    for item in transcript_dict[participant]:\n",
    "        # Split each content string into its component sentences.\n",
    "        # TODO: Stemming and apostrophe mgmt ('  's  'nt  &c.).  Doing so will boost scores for sentences with \"hub's\" vice \"hub\"\n",
    "        transcript_dict[participant][item].setdefault('sentencized_content',nltk.sent_tokenize(transcript_dict[participant][item]['content'].lower()))\n",
    "        transcript_dict[participant][item].setdefault('stripped_tokenized_content',stripStopwords(transcript_dict[participant][item]['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_word_freq = {}\n",
    "item_word_freq = {}\n",
    "overall_word_freq = {}\n",
    "for participant in transcript_dict:\n",
    "    participant_word_freq.setdefault(participant,dict())\n",
    "    for item in transcript_dict[participant]:\n",
    "        item_word_freq.setdefault(item,dict())\n",
    "        for word in transcript_dict[participant][item]['stripped_tokenized_content']:\n",
    "            if word not in item_word_freq[item].keys():\n",
    "                item_word_freq[item][word] = 1\n",
    "            else:\n",
    "                item_word_freq[item][word] += 1\n",
    "            if word not in participant_word_freq[participant].keys():\n",
    "                participant_word_freq[participant][word] = 1\n",
    "            else:\n",
    "                participant_word_freq[participant][word] += 1\n",
    "            if word not in overall_word_freq.keys():\n",
    "                overall_word_freq[word] = 1\n",
    "            else:\n",
    "                overall_word_freq[word] += 1\n",
    "#             print(item_word_freq)\n",
    "        item_max_freq = max(item_word_freq[item].values())\n",
    "        for word in item_word_freq[item].keys():\n",
    "            item_word_freq[item][word] = (item_word_freq[item][word]/item_max_freq)\n",
    "#         transcript_dict[participant][item].setdefault('content_word_frequencies',item_word_freq)\n",
    "    part_max_freq = max(participant_word_freq[participant].values())\n",
    "    for word in participant_word_freq[participant].keys():\n",
    "        participant_word_freq[participant][word] = (participant_word_freq[participant][word]/part_max_freq)\n",
    "overall_max_freq = max(overall_word_freq.values())\n",
    "for word in overall_word_freq.keys():\n",
    "    overall_word_freq[word] = (overall_word_freq[word]/overall_max_freq)\n",
    "#     transcript_dict[participant].setdefault('participant_word_frequencies',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF using just the sentence-tokenized raw content with no other preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "basic_sent_dict = nltk.sent_tokenize(raw.lower())\n",
    "for sent in basic_sent_dict:\n",
    "    for word in nltk.word_tokenize(sent):\n",
    "        if word in overall_word_freq.keys():\n",
    "            if len(sent.split(' '))<30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = overall_word_freq[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += overall_word_freq[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sentences = heapq.nlargest(10, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)\n",
    "\n",
    "# if you want a laff just\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF using preprocessed data from transcript_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "\n",
    "\n",
    "\n",
    "for participant in transcript_dict:\n",
    "    for item in transcript_dict[participant]:\n",
    "        for sent in transcript_dict[participant][item]['sentencized_content']:\n",
    "            for word in nltk.word_tokenize(sent):\n",
    "                if word in overall_word_freq.keys():\n",
    "                    if len(sent.split(' '))<30 and len(sent.split(' '))>3:\n",
    "                        if sent not in sentence_scores.keys():\n",
    "                            sentence_scores[sent] = overall_word_freq[word]\n",
    "                        else:\n",
    "                            sentence_scores[sent] += overall_word_freq[word]\n",
    "\n",
    "# print(sentence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los angeles, seattle, chicago, atlanta, most of the major cities in fact are very tight on capacity for intermodal. i would suggest it'll free up, particularly in some of the very constrained cities that i'd mentioned earlier in the call, such as los angeles, seattle, chicago. and then the cost and expenses, we every quarter had the bonus, $7.5 million higher than the prior year, because we're doing very well this year. so the mix of our business has changed, but we've also done â€“ as you know the market changed, jason, last year. it's probably in the 15%, 20%, maybe 25% range, the differential. i don't â€“ yeah, so with logistics, maybe to define it better, we kind of lost some business due to bankruptcy and insourcing. we have seen the truck industry, the over the road, that has softened a little bit, but the intermodal's been extraordinarily strong. our targeted approach has allowed us to grow both revenue and yield, while we also focus on process, workflow engineering, network improvements and execution. we did, however, experience higher cost due to start-up expenses, driver wage increases and greater use of outside carriers. so the service is, in large part, in my mind, due to the increases in volume.\n"
     ]
    }
   ],
   "source": [
    "summary_sentences = heapq.nlargest(10, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)\n",
    "\n",
    "# if you want a laff just\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
